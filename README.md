# nlp-security
Enhancing Bias via Backdoor Attacks in Traditional and Transformer Text Classification Models

The robustness of NLP models has been under investigation by researchers, encompassing their susceptibility to adversarial attacks, which involve manipulating input data with the intent to trick models into producing inaccurate predictions or classifications. This article aims to unravel vulnerabilities introduced by backdoor attacks from a fairness perspective. It demonstrates how adversarial attacks can be used to target certain demographic groups or generate biased outputs, perpetuating or even amplifying the bias in the model's predictions. To test how NLP models are susceptible to backdoor attacks on amplifying bias, two main pipelines are  implemented in which NLP models are compared with baseline conventional machine learning models using the same adversarial attack strategy by injecting poisoned bias-enhancing triggers and analyzing the effect of such backdoor attack. To evaluate the success of backdoor attacks on NLP models, a newly proposed metric called Backdoor Bias Success Rate (BBSR) is introduced. This metric is used to assess the effectiveness of injecting bias-enhancing triggers. Finally, we conclude that it is important to consider both robustness and fairness in designing and evaluating NLP models to ensure ethical, secure, and effective applications.
